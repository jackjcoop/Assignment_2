{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d6c8acd-0e81-420e-ba09-915487cc61cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5016 | Train Acc: 82.26%\n",
      "  Val   Loss: 0.3525 | Val   Acc: 87.22%\n",
      "  Confusion Matrix:\n",
      "[[326  23   1  22]\n",
      " [ 67 653  36  12]\n",
      " [ 10  13 324   5]\n",
      " [ 16  23   2 267]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2522 | Train Acc: 91.19%\n",
      "  Val   Loss: 0.3399 | Val   Acc: 87.33%\n",
      "  Confusion Matrix:\n",
      "[[333  13   5  21]\n",
      " [ 71 639  40  18]\n",
      " [  7  12 329   4]\n",
      " [ 20  14   3 271]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1975 | Train Acc: 93.26%\n",
      "  Val   Loss: 0.3097 | Val   Acc: 90.22%\n",
      "  Confusion Matrix:\n",
      "[[329  23   1  19]\n",
      " [ 35 700  12  21]\n",
      " [ 10  21 316   5]\n",
      " [ 13  15   1 279]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1627 | Train Acc: 94.52%\n",
      "  Val   Loss: 0.3213 | Val   Acc: 89.22%\n",
      "  Confusion Matrix:\n",
      "[[311  29   9  23]\n",
      " [ 27 692  26  23]\n",
      " [  5  18 325   4]\n",
      " [ 12  14   4 278]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Test Results\n",
      "  Test Loss:   0.4562 | Accuracy: 85.40%\n",
      "  Confusion Matrix:\n",
      "[[475  98  36  86]\n",
      " [ 47 967  47  25]\n",
      " [ 12  20 764   3]\n",
      " [ 52  59  16 725]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition (Gated Fusion with ResNet50)\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Intermediate Fusion: Concatenation + MLP ----\n",
    "        # Concatenates text and image features (resulting in 2*fusion_dim) and fuses via MLP.\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fusion_dim, fusion_dim)\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Intermediate Fusion: Concatenation + MLP ---\n",
    "        # Concatenate along the feature dimension\n",
    "        multimodal_feats = torch.cat((text_feat, image_feat), dim=1)  # (batch, fusion_dim*2)\n",
    "        fused_feat = self.fusion_mlp(multimodal_feats)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation with Enhanced Augmentation\n",
    "# ======================\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as before\n",
    "\n",
    "# Define dataset paths.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# ======================\n",
    "# Create DataLoaders with Balancing for Training Data\n",
    "# ======================\n",
    "# Compute sample weights based on class frequencies in the training set.\n",
    "train_labels = [sample[2] for sample in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts  # Inverse frequency for each class.\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create the sampler for balanced sampling.\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Use the sampler in the training DataLoader.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the gating fusion model with ResNet50.\n",
    "model = MultiModalClassifierGated(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler (without verbose parameter).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513c8ba-1abd-407e-bc41-e34c93809096",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.5016 | Train Acc: 82.26%\n",
    "  Val   Loss: 0.3525 | Val   Acc: 87.22%\n",
    "  Confusion Matrix:\n",
    "[[326  23   1  22]\n",
    " [ 67 653  36  12]\n",
    " [ 10  13 324   5]\n",
    " [ 16  23   2 267]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2522 | Train Acc: 91.19%\n",
    "  Val   Loss: 0.3399 | Val   Acc: 87.33%\n",
    "  Confusion Matrix:\n",
    "[[333  13   5  21]\n",
    " [ 71 639  40  18]\n",
    " [  7  12 329   4]\n",
    " [ 20  14   3 271]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1975 | Train Acc: 93.26%\n",
    "  Val   Loss: 0.3097 | Val   Acc: 90.22%\n",
    "  Confusion Matrix:\n",
    "[[329  23   1  19]\n",
    " [ 35 700  12  21]\n",
    " [ 10  21 316   5]\n",
    " [ 13  15   1 279]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1627 | Train Acc: 94.52%\n",
    "  Val   Loss: 0.3213 | Val   Acc: 89.22%\n",
    "  Confusion Matrix:\n",
    "[[311  29   9  23]\n",
    " [ 27 692  26  23]\n",
    " [  5  18 325   4]\n",
    " [ 12  14   4 278]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Test Results\n",
    "  Test Loss:   0.4562 | Accuracy: 85.40%\n",
    "  Confusion Matrix:\n",
    "[[475  98  36  86]\n",
    " [ 47 967  47  25]\n",
    " [ 12  20 764   3]\n",
    " [ 52  59  16 725]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efdc92a9-8d33-47d6-ba27-c40ecdca3f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc8f138-d93e-4fe0-8f37-e75377c7e947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5373 | Train Acc: 79.74%\n",
      "  Val   Loss: 0.3192 | Val   Acc: 88.44%\n",
      "  Confusion Matrix:\n",
      "[[299  47   4  22]\n",
      " [ 31 711  15  11]\n",
      " [  6  21 322   3]\n",
      " [ 24  22   2 260]]\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2649 | Train Acc: 90.67%\n",
      "  Val   Loss: 0.2952 | Val   Acc: 89.78%\n",
      "  Confusion Matrix:\n",
      "[[318  34   1  19]\n",
      " [ 33 711   8  16]\n",
      " [  4  26 317   5]\n",
      " [ 21  17   0 270]]\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1989 | Train Acc: 92.98%\n",
      "  Val   Loss: 0.3014 | Val   Acc: 89.94%\n",
      "  Confusion Matrix:\n",
      "[[333  23   1  15]\n",
      " [ 42 695  14  17]\n",
      " [  7  21 321   3]\n",
      " [ 22  16   0 270]]\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1586 | Train Acc: 94.27%\n",
      "  Val   Loss: 0.2869 | Val   Acc: 90.33%\n",
      "  Confusion Matrix:\n",
      "[[329  29   1  13]\n",
      " [ 30 710  17  11]\n",
      " [  4  22 322   4]\n",
      " [ 21  22   0 265]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.4506 | Accuracy: 85.23%\n",
      "  Confusion Matrix:\n",
      "[[516 108  15  56]\n",
      " [ 46 989  38  13]\n",
      " [ 24  26 745   4]\n",
      " [ 69  90  18 675]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze most image model parameters except those in the last transformer block.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        # Project DistilBERT's output (768-dim) to fusion_dim.\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Image model: Vision Transformer (ViT) ----\n",
    "        self.image_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        # Remove the classification head to obtain features.\n",
    "        self.image_model.heads = nn.Identity()\n",
    "        \n",
    "        if freeze_image_layers:\n",
    "            # Freeze all parameters except for those in the final transformer block.\n",
    "            # Note: vit_b_16 has 12 transformer blocks stored in encoder.layers.\n",
    "            last_block_index = len(self.image_model.encoder.layers) - 1\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if 'encoder.layers' in name:\n",
    "                    # Expected format: \"encoder.layers.encoder_layer_<index>....\"\n",
    "                    block_index = int(name.split('.')[2].split('_')[-1])\n",
    "                    if block_index != last_block_index:\n",
    "                        param.requires_grad = False\n",
    "                else:\n",
    "                    # Optionally freeze non-encoder parameters (e.g. patch embedding, positional encoding)\n",
    "                    param.requires_grad = False\n",
    "\n",
    "\n",
    "        \n",
    "        # The ViT model outputs embeddings of size 768 by default.\n",
    "        if 768 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(768, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Attention-based Fusion ----\n",
    "        # We stack text and image embeddings (each of size fusion_dim) into a sequence of 2 tokens.\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=fusion_dim, num_heads=1, batch_first=True)\n",
    "        self.fusion_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token) as text feature.\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # (batch, 768)\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 768)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Stack features and apply attention fusion ---\n",
    "        # Create a sequence of two tokens per sample: one from text and one from image.\n",
    "        multimodal_feats = torch.stack([text_feat, image_feat], dim=1)  # (batch, 2, fusion_dim)\n",
    "        # Self-attention: query, key, and value are the same.\n",
    "        attn_output, _ = self.attention(multimodal_feats, multimodal_feats, multimodal_feats)\n",
    "        # Aggregate the two tokens by averaging over the sequence dimension.\n",
    "        fused_feat = attn_output.mean(dim=1)  # (batch, fusion_dim)\n",
    "        fused_feat = self.fusion_dropout(fused_feat)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation\n",
    "# ======================\n",
    "# Define transforms for training and validation (you can adjust augmentation as needed)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss and accuracy\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Updated training loop with progress evaluation\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Optionally, run on test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1b7e1-14ec-4a1a-8941-dab23b2d31e6",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.5373 | Train Acc: 79.74%\n",
    "  Val   Loss: 0.3192 | Val   Acc: 88.44%\n",
    "  Confusion Matrix:\n",
    "[[299  47   4  22]\n",
    " [ 31 711  15  11]\n",
    " [  6  21 322   3]\n",
    " [ 24  22   2 260]]\n",
    "\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2649 | Train Acc: 90.67%\n",
    "  Val   Loss: 0.2952 | Val   Acc: 89.78%\n",
    "  Confusion Matrix:\n",
    "[[318  34   1  19]\n",
    " [ 33 711   8  16]\n",
    " [  4  26 317   5]\n",
    " [ 21  17   0 270]]\n",
    "\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1989 | Train Acc: 92.98%\n",
    "  Val   Loss: 0.3014 | Val   Acc: 89.94%\n",
    "  Confusion Matrix:\n",
    "[[333  23   1  15]\n",
    " [ 42 695  14  17]\n",
    " [  7  21 321   3]\n",
    " [ 22  16   0 270]]\n",
    "\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1586 | Train Acc: 94.27%\n",
    "  Val   Loss: 0.2869 | Val   Acc: 90.33%\n",
    "  Confusion Matrix:\n",
    "[[329  29   1  13]\n",
    " [ 30 710  17  11]\n",
    " [  4  22 322   4]\n",
    " [ 21  22   0 265]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.4506 | Accuracy: 85.23%\n",
    "  Confusion Matrix:\n",
    "[[516 108  15  56]\n",
    " [ 46 989  38  13]\n",
    " [ 24  26 745   4]\n",
    " [ 69  90  18 675]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ed1a83a-e39a-45c7-b10b-3412c556ea42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5098 | Train Acc: 80.93%\n",
      "  Val   Loss: 0.3196 | Val   Acc: 88.61%\n",
      "  Confusion Matrix:\n",
      "[[305  46   1  20]\n",
      " [ 31 716  15   6]\n",
      " [  9  23 317   3]\n",
      " [ 24  27   0 257]]\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2649 | Train Acc: 90.41%\n",
      "  Val   Loss: 0.2830 | Val   Acc: 89.94%\n",
      "  Confusion Matrix:\n",
      "[[327  26   2  17]\n",
      " [ 35 710  12  11]\n",
      " [  5  21 322   4]\n",
      " [ 29  19   0 260]]\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2037 | Train Acc: 92.89%\n",
      "  Val   Loss: 0.3050 | Val   Acc: 90.06%\n",
      "  Confusion Matrix:\n",
      "[[333  28   1  10]\n",
      " [ 35 707  18   8]\n",
      " [  4  19 326   3]\n",
      " [ 34  19   0 255]]\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1477 | Train Acc: 94.81%\n",
      "  Val   Loss: 0.2943 | Val   Acc: 90.11%\n",
      "  Confusion Matrix:\n",
      "[[314  36   4  18]\n",
      " [ 24 717  15  12]\n",
      " [  2  20 326   4]\n",
      " [ 21  21   1 265]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.4373 | Accuracy: 85.69%\n",
      "  Confusion Matrix:\n",
      "[[ 499  121   19   56]\n",
      " [  30 1002   39   15]\n",
      " [  17   25  751    6]\n",
      " [  58   85   20  689]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze most image model parameters except for the last two transformer blocks.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Image model: Vision Transformer (ViT) ----\n",
    "        self.image_model = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.heads = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        if freeze_image_layers:\n",
    "            # Unfreeze the last two transformer blocks; freeze earlier ones.\n",
    "            last_block_index = len(self.image_model.encoder.layers) - 1\n",
    "            second_last_index = last_block_index - 1\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if 'encoder.layers' in name:\n",
    "                    # Expecting names like \"encoder.layers.encoder_layer_0....\"\n",
    "                    block_str = name.split('.')[2]\n",
    "                    if block_str.startswith('encoder_layer_'):\n",
    "                        block_index = int(block_str.split('_')[-1])\n",
    "                    else:\n",
    "                        block_index = int(block_str)\n",
    "                    if block_index not in [second_last_index, last_block_index]:\n",
    "                        param.requires_grad = False\n",
    "                else:\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        if 768 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(768, fusion_dim)\n",
    "        else:            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Gating Mechanism for Fusion ----\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fusion_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Process text\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)\n",
    "        \n",
    "        # Process image\n",
    "        image_feat = self.image_model(image)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)\n",
    "        \n",
    "        # Gated Fusion\n",
    "        gate_weights = self.gate(torch.cat([text_feat, image_feat], dim=1))\n",
    "        fused_feat = gate_weights * text_feat + (1 - gate_weights) * image_feat\n",
    "        fused_feat = self.fusion_dropout(fused_feat)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation\n",
    "# ======================\n",
    "# Define transforms for training and validation (adjust augmentations as needed)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Evaluate on test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5b31b-de44-4e35-9f1b-0f04e03e9c2d",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.5098 | Train Acc: 80.93%\n",
    "  Val   Loss: 0.3196 | Val   Acc: 88.61%\n",
    "  Confusion Matrix:\n",
    "[[305  46   1  20]\n",
    " [ 31 716  15   6]\n",
    " [  9  23 317   3]\n",
    " [ 24  27   0 257]]\n",
    "\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2649 | Train Acc: 90.41%\n",
    "  Val   Loss: 0.2830 | Val   Acc: 89.94%\n",
    "  Confusion Matrix:\n",
    "[[327  26   2  17]\n",
    " [ 35 710  12  11]\n",
    " [  5  21 322   4]\n",
    " [ 29  19   0 260]]\n",
    "\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2037 | Train Acc: 92.89%\n",
    "  Val   Loss: 0.3050 | Val   Acc: 90.06%\n",
    "  Confusion Matrix:\n",
    "[[333  28   1  10]\n",
    " [ 35 707  18   8]\n",
    " [  4  19 326   3]\n",
    " [ 34  19   0 255]]\n",
    "\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1477 | Train Acc: 94.81%\n",
    "  Val   Loss: 0.2943 | Val   Acc: 90.11%\n",
    "  Confusion Matrix:\n",
    "[[314  36   4  18]\n",
    " [ 24 717  15  12]\n",
    " [  2  20 326   4]\n",
    " [ 21  21   1 265]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.4373 | Accuracy: 85.69%\n",
    "  Confusion Matrix:\n",
    "[[ 499  121   19   56]\n",
    " [  30 1002   39   15]\n",
    " [  17   25  751    6]\n",
    " [  58   85   20  689]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:enen_645]",
   "language": "python",
   "name": "conda-env-enen_645-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
