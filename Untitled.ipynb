{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227949b-9d66-4a4c-adc5-b3e8c16e1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights # move these to where it is actually used \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694ca9c-a40d-4b65-b428-1c9e22dbc7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc2983-03bb-410b-b426-0106216e12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition (Gated Fusion with ResNet50)\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Intermediate Fusion: Concatenation + MLP ----\n",
    "        # Concatenates text and image features (resulting in 2*fusion_dim) and fuses via MLP.\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fusion_dim, fusion_dim)\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Intermediate Fusion: Concatenation + MLP ---\n",
    "        # Concatenate along the feature dimension\n",
    "        multimodal_feats = torch.cat((text_feat, image_feat), dim=1)  # (batch, fusion_dim*2)\n",
    "        fused_feat = self.fusion_mlp(multimodal_feats)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c183dfe-6643-4e66-a7c0-aba13ef9ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 3. Data Preparation with Enhanced Augmentation\n",
    "# ======================\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as before\n",
    "\n",
    "# Define dataset paths.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34971a42-d15d-492b-9c39-e1804df260a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# Create DataLoaders with Balancing for Training Data\n",
    "# ======================\n",
    "# Compute sample weights based on class frequencies in the training set.\n",
    "train_labels = [sample[2] for sample in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts  # Inverse frequency for each class.\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create the sampler for balanced sampling.\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Use the sampler in the training DataLoader.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce56b0d-6e49-4ab2-8165-08a861565a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the gating fusion model with ResNet50.\n",
    "model = MultiModalClassifierGated(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler (without verbose parameter).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c3c7a-08f9-457a-8848-42772834134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:enen_645]",
   "language": "python",
   "name": "conda-env-enen_645-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
