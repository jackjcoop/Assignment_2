{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbf1ef-5041-46f9-a7c8-0c17447f2706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self attentition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b541695f-c58a-46a2-be56-4f88b70ac086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started next Epoch\n",
      "Epoch 1/4\n",
      "  Train Loss: 0.4570\n",
      "  Val Loss:   0.3340 | Accuracy: 87.17%\n",
      "  Confusion Matrix:\n",
      "[[333  22   3  14]\n",
      " [ 68 674  15  11]\n",
      " [ 10  20 319   3]\n",
      " [ 49  15   1 243]]\n",
      "\n",
      "Started next Epoch\n",
      "Epoch 2/4\n",
      "  Train Loss: 0.2606\n",
      "  Val Loss:   0.3123 | Accuracy: 89.72%\n",
      "  Confusion Matrix:\n",
      "[[329  31   1  11]\n",
      " [ 35 718   7   8]\n",
      " [  7  20 321   4]\n",
      " [ 40  21   0 247]]\n",
      "\n",
      "Started next Epoch\n",
      "Epoch 3/4\n",
      "  Train Loss: 0.1973\n",
      "  Val Loss:   0.2871 | Accuracy: 90.11%\n",
      "  Confusion Matrix:\n",
      "[[328  30   2  12]\n",
      " [ 33 706  16  13]\n",
      " [  7  21 320   4]\n",
      " [ 23  17   0 268]]\n",
      "\n",
      "Started next Epoch\n",
      "Epoch 4/4\n",
      "  Train Loss: 0.1560\n",
      "  Val Loss:   0.3175 | Accuracy: 90.39%\n",
      "  Confusion Matrix:\n",
      "[[321  27   8  16]\n",
      " [ 28 711  20   9]\n",
      " [  4  17 328   3]\n",
      " [ 18  18   5 267]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.4996 | Accuracy: 85.02%\n",
      "  Confusion Matrix:\n",
      "[[500 101  33  61]\n",
      " [ 51 982  37  16]\n",
      " [ 15  20 761   3]\n",
      " [ 64  88  25 675]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        # Project DistilBERT's output (768-dim) to fusion_dim.\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        # Remove the final fc layer to obtain features (ResNet18 outputs 512-dim).\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            # Freeze all parameters except those in layer4 (or adjust as needed)\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        # If necessary, project image features to fusion_dim.\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Attention-based Fusion ----\n",
    "        # We stack text and image embeddings (each of size fusion_dim) into a sequence of 2 tokens.\n",
    "        # Using a single-head self-attention (batch_first=True so that input is (B, SeqLen, fusion_dim)).\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=fusion_dim, num_heads=1, batch_first=True)\n",
    "        self.fusion_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token) as text feature.\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # (batch, 768)\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Stack features and apply attention fusion ---\n",
    "        # Create a sequence of two tokens per sample: one from text and one from image.\n",
    "        multimodal_feats = torch.stack([text_feat, image_feat], dim=1)  # (batch, 2, fusion_dim)\n",
    "        # Self-attention: here query, key, and value are the same.\n",
    "        attn_output, attn_weights = self.attention(multimodal_feats, multimodal_feats, multimodal_feats)\n",
    "        # Aggregate the two tokens by averaging over the sequence dimension.\n",
    "        fused_feat = attn_output.mean(dim=1)  # (batch, fusion_dim)\n",
    "        fused_feat = self.fusion_dropout(fused_feat)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation\n",
    "# ======================\n",
    "# Define transforms for training and validation (you can adjust augmentation as needed)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss and accuracy\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Updated training loop with progress evaluation\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Optionally, run on test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae4da7-04ac-4617-a7c6-5223f119ba81",
   "metadata": {},
   "source": [
    "Started next Epoch\n",
    "Epoch 1/4\n",
    "  Train Loss: 0.4570\n",
    "  Val Loss:   0.3340 | Accuracy: 87.17%\n",
    "  Confusion Matrix:\n",
    "[[333  22   3  14]\n",
    " [ 68 674  15  11]\n",
    " [ 10  20 319   3]\n",
    " [ 49  15   1 243]]\n",
    "\n",
    "Started next Epoch\n",
    "Epoch 2/4\n",
    "  Train Loss: 0.2606\n",
    "  Val Loss:   0.3123 | Accuracy: 89.72%\n",
    "  Confusion Matrix:\n",
    "[[329  31   1  11]\n",
    " [ 35 718   7   8]\n",
    " [  7  20 321   4]\n",
    " [ 40  21   0 247]]\n",
    "\n",
    "Started next Epoch\n",
    "Epoch 3/4\n",
    "  Train Loss: 0.1973\n",
    "  Val Loss:   0.2871 | Accuracy: 90.11%\n",
    "  Confusion Matrix:\n",
    "[[328  30   2  12]\n",
    " [ 33 706  16  13]\n",
    " [  7  21 320   4]\n",
    " [ 23  17   0 268]]\n",
    "\n",
    "Started next Epoch\n",
    "Epoch 4/4\n",
    "  Train Loss: 0.1560\n",
    "  Val Loss:   0.3175 | Accuracy: 90.39%\n",
    "  Confusion Matrix:\n",
    "[[321  27   8  16]\n",
    " [ 28 711  20   9]\n",
    " [  4  17 328   3]\n",
    " [ 18  18   5 267]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.4996 | Accuracy: 85.02%\n",
    "  Confusion Matrix:\n",
    "[[500 101  33  61]\n",
    " [ 51 982  37  16]\n",
    " [ 15  20 761   3]\n",
    " [ 64  88  25 675]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98a1787-b38e-42c1-9f92-6b7020ab568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1c164c-46dd-4fee-8e4e-afe1f93c4e0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.4989 | Train Acc: 81.43%\n",
      "  Val   Loss: 0.3339 | Val   Acc: 88.17%\n",
      "  Confusion Matrix:\n",
      "[[300  41  10  21]\n",
      " [ 41 691  26  10]\n",
      " [  4  17 328   3]\n",
      " [ 17  17   6 268]]\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2668 | Train Acc: 90.78%\n",
      "  Val   Loss: 0.2975 | Val   Acc: 89.72%\n",
      "  Confusion Matrix:\n",
      "[[325  29   1  17]\n",
      " [ 37 712  10   9]\n",
      " [ 10  21 317   4]\n",
      " [ 24  23   0 261]]\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1959 | Train Acc: 93.29%\n",
      "  Val   Loss: 0.2983 | Val   Acc: 90.17%\n",
      "  Confusion Matrix:\n",
      "[[334  20   4  14]\n",
      " [ 47 702   7  12]\n",
      " [  8  26 314   4]\n",
      " [ 23  12   0 273]]\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1569 | Train Acc: 94.48%\n",
      "  Val   Loss: 0.3457 | Val   Acc: 89.56%\n",
      "  Confusion Matrix:\n",
      "[[307  39   6  20]\n",
      " [ 26 710  22  10]\n",
      " [  4  19 325   4]\n",
      " [ 16  19   3 270]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.4953 | Accuracy: 85.20%\n",
      "  Confusion Matrix:\n",
      "[[476 117  33  69]\n",
      " [ 36 989  44  17]\n",
      " [ 10  25 760   4]\n",
      " [ 51  78  24 699]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition\n",
    "# ======================\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Intermediate Fusion: Concatenation + MLP ----\n",
    "        # Concatenates text and image features (resulting in 2*fusion_dim) and fuses via MLP.\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim, fusion_dim)\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token representation\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Intermediate Fusion: Concatenation + MLP ---\n",
    "        # Concatenate along the feature dimension\n",
    "        multimodal_feats = torch.cat((text_feat, image_feat), dim=1)  # (batch, fusion_dim*2)\n",
    "        fused_feat = self.fusion_mlp(multimodal_feats)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation\n",
    "# ======================\n",
    "# Define transforms for training and validation (you can adjust augmentation as needed)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss and accuracy\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Updated training loop with progress evaluation\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Optionally, run on test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd018c-6425-490f-81c2-714876ea1b9b",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.4989 | Train Acc: 81.43%\n",
    "  Val   Loss: 0.3339 | Val   Acc: 88.17%\n",
    "  Confusion Matrix:\n",
    "[[300  41  10  21]\n",
    " [ 41 691  26  10]\n",
    " [  4  17 328   3]\n",
    " [ 17  17   6 268]]\n",
    "\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2668 | Train Acc: 90.78%\n",
    "  Val   Loss: 0.2975 | Val   Acc: 89.72%\n",
    "  Confusion Matrix:\n",
    "[[325  29   1  17]\n",
    " [ 37 712  10   9]\n",
    " [ 10  21 317   4]\n",
    " [ 24  23   0 261]]\n",
    "\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1959 | Train Acc: 93.29%\n",
    "  Val   Loss: 0.2983 | Val   Acc: 90.17%\n",
    "  Confusion Matrix:\n",
    "[[334  20   4  14]\n",
    " [ 47 702   7  12]\n",
    " [  8  26 314   4]\n",
    " [ 23  12   0 273]]\n",
    "\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1569 | Train Acc: 94.48%\n",
    "  Val   Loss: 0.3457 | Val   Acc: 89.56%\n",
    "  Confusion Matrix:\n",
    "[[307  39   6  20]\n",
    " [ 26 710  22  10]\n",
    " [  4  19 325   4]\n",
    " [ 16  19   3 270]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.4953 | Accuracy: 85.20%\n",
    "  Confusion Matrix:\n",
    "[[476 117  33  69]\n",
    " [ 36 989  44  17]\n",
    " [ 10  25 760   4]\n",
    " [ 51  78  24 699]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d623e0-5793-4bbc-a69e-15dbeddcffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilinear pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774f9c6a-ac78-40aa-b460-00efe6c18b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.4767 | Train Acc: 82.91%\n",
      "  Val   Loss: 0.3170 | Val   Acc: 88.39%\n",
      "  Confusion Matrix:\n",
      "[[285  60   2  25]\n",
      " [ 24 714  15  15]\n",
      " [  1  27 320   4]\n",
      " [ 16  19   1 272]]\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2635 | Train Acc: 90.90%\n",
      "  Val   Loss: 0.2875 | Val   Acc: 90.11%\n",
      "  Confusion Matrix:\n",
      "[[305  50   2  15]\n",
      " [ 25 726   8   9]\n",
      " [  3  22 324   3]\n",
      " [ 18  21   2 267]]\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2024 | Train Acc: 92.79%\n",
      "  Val   Loss: 0.2992 | Val   Acc: 90.61%\n",
      "  Confusion Matrix:\n",
      "[[327  27   2  16]\n",
      " [ 33 716  11   8]\n",
      " [  4  19 325   4]\n",
      " [ 21  23   1 263]]\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1627 | Train Acc: 94.37%\n",
      "  Val   Loss: 0.3404 | Val   Acc: 88.78%\n",
      "  Confusion Matrix:\n",
      "[[342  16   3  11]\n",
      " [ 56 688  16   8]\n",
      " [  6  20 322   4]\n",
      " [ 42  19   1 246]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.5367 | Accuracy: 83.77%\n",
      "  Confusion Matrix:\n",
      "[[599  64  18  14]\n",
      " [ 99 941  39   7]\n",
      " [ 32  22 742   3]\n",
      " [148  96  15 593]]\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition\n",
    "# ======================\n",
    "class MultiModalClassifierBilinear(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifierBilinear, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Bilinear Pooling Fusion ----\n",
    "        # The bilinear layer learns a transformation: fused = x^T W y + b,\n",
    "        # where x and y are the projected text and image features.\n",
    "        self.bilinear = nn.Bilinear(fusion_dim, fusion_dim, fusion_dim)\n",
    "        self.pool_dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token) as text feature.\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # (batch, 768)\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Bilinear Fusion ---\n",
    "        fused_feat = self.bilinear(text_feat, image_feat)  # (batch, fusion_dim)\n",
    "        fused_feat = self.pool_dropout(fused_feat)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation\n",
    "# ======================\n",
    "# Define transforms for training and validation (you can adjust augmentation as needed)\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate batch accuracy\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss and accuracy\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Updated training loop with progress evaluation\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Optionally, run on test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f3059-17d7-4dc7-8d2f-7e7bbd875b93",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.4767 | Train Acc: 82.91%\n",
    "  Val   Loss: 0.3170 | Val   Acc: 88.39%\n",
    "  Confusion Matrix:\n",
    "[[285  60   2  25]\n",
    " [ 24 714  15  15]\n",
    " [  1  27 320   4]\n",
    " [ 16  19   1 272]]\n",
    "\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2635 | Train Acc: 90.90%\n",
    "  Val   Loss: 0.2875 | Val   Acc: 90.11%\n",
    "  Confusion Matrix:\n",
    "[[305  50   2  15]\n",
    " [ 25 726   8   9]\n",
    " [  3  22 324   3]\n",
    " [ 18  21   2 267]]\n",
    "\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2024 | Train Acc: 92.79%\n",
    "  Val   Loss: 0.2992 | Val   Acc: 90.61%\n",
    "  Confusion Matrix:\n",
    "[[327  27   2  16]\n",
    " [ 33 716  11   8]\n",
    " [  4  19 325   4]\n",
    " [ 21  23   1 263]]\n",
    "\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1627 | Train Acc: 94.37%\n",
    "  Val   Loss: 0.3404 | Val   Acc: 88.78%\n",
    "  Confusion Matrix:\n",
    "[[342  16   3  11]\n",
    " [ 56 688  16   8]\n",
    " [  6  20 322   4]\n",
    " [ 42  19   1 246]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.5367 | Accuracy: 83.77%\n",
    "  Confusion Matrix:\n",
    "[[599  64  18  14]\n",
    " [ 99 941  39   7]\n",
    " [ 32  22 742   3]\n",
    " [148  96  15 593]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c2fe1-0eac-480e-bc83-fa2c783b8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gating mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e9a76a4-cd75-44cf-b7af-872c7f59b24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacks/miniconda3/envs/enen_645/lib/python3.13/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.5415 | Train Acc: 80.17%\n",
      "  Val   Loss: 0.3330 | Val   Acc: 87.94%\n",
      "  Confusion Matrix:\n",
      "[[298  51   6  17]\n",
      " [ 32 710  17   9]\n",
      " [  4  22 323   3]\n",
      " [ 29  22   5 252]]\n",
      "\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2960 | Train Acc: 89.71%\n",
      "  Val   Loss: 0.3107 | Val   Acc: 88.61%\n",
      "  Confusion Matrix:\n",
      "[[313  34   3  22]\n",
      " [ 36 675  31  26]\n",
      " [  6  17 326   3]\n",
      " [ 15   8   4 281]]\n",
      "\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2313 | Train Acc: 91.86%\n",
      "  Val   Loss: 0.3369 | Val   Acc: 89.00%\n",
      "  Confusion Matrix:\n",
      "[[316  28   1  27]\n",
      " [ 43 690   5  30]\n",
      " [ 11  27 309   5]\n",
      " [ 12   9   0 287]]\n",
      "\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1926 | Train Acc: 93.02%\n",
      "  Val   Loss: 0.3289 | Val   Acc: 89.61%\n",
      "  Confusion Matrix:\n",
      "[[303  49   3  17]\n",
      " [ 25 707  20  16]\n",
      " [  5  16 327   4]\n",
      " [ 10  18   4 276]]\n",
      "\n",
      "Test Results\n",
      "  Test Loss:   0.4768 | Accuracy: 84.82%\n",
      "  Confusion Matrix:\n",
      "[[442 151  28  74]\n",
      " [ 29 995  40  22]\n",
      " [ 10  21 762   6]\n",
      " [ 45  78  17 712]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition (Gated Fusion)\n",
    "# ======================\n",
    "class MultiModalClassifierGated(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifierGated, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Image model: ResNet18 ----\n",
    "        self.image_model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        if 512 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(512, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Gating Mechanism ----\n",
    "        # The gate network computes element-wise weights for text and image features.\n",
    "        # It takes the concatenated features (of size fusion_dim*2) and outputs gating weights in [0, 1]\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the [CLS] token representation (first token) as text feature.\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # (batch, 768)\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # (batch, 512)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Gating Fusion ---\n",
    "        # Concatenate features from both modalities.\n",
    "        combined_feats = torch.cat([text_feat, image_feat], dim=1)  # (batch, fusion_dim*2)\n",
    "        # Compute gate weights (element-wise) from the combined features.\n",
    "        gate_weights = self.gate(combined_feats)  # (batch, fusion_dim)\n",
    "        # Fuse the modalities by weighting text and image features.\n",
    "        fused_feat = gate_weights * text_feat + (1 - gate_weights) * image_feat  # (batch, fusion_dim)\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation with Enhanced Augmentation\n",
    "# ======================\n",
    "# Enhanced training transforms with additional augmentation.\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instantiate tokenizer for DistilBERT.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as in your example\n",
    "\n",
    "# Define paths to your train, validation, and test folders.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# Create dataloaders.\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "# Instantiate the gating fusion model.\n",
    "model = MultiModalClassifierGated(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Using weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler: ReduceLROnPlateau reduces the LR when the validation loss plateaus.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss and accuracy\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "# Updated training loop with scheduler integration\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    # Step the scheduler based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save the best model based on validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "# Evaluate on the test set.\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5664b-1867-4473-b460-958ecce77261",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.5415 | Train Acc: 80.17%\n",
    "  Val   Loss: 0.3330 | Val   Acc: 87.94%\n",
    "  Confusion Matrix:\n",
    "[[298  51   6  17]\n",
    " [ 32 710  17   9]\n",
    " [  4  22 323   3]\n",
    " [ 29  22   5 252]]\n",
    "\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2960 | Train Acc: 89.71%\n",
    "  Val   Loss: 0.3107 | Val   Acc: 88.61%\n",
    "  Confusion Matrix:\n",
    "[[313  34   3  22]\n",
    " [ 36 675  31  26]\n",
    " [  6  17 326   3]\n",
    " [ 15   8   4 281]]\n",
    "\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2313 | Train Acc: 91.86%\n",
    "  Val   Loss: 0.3369 | Val   Acc: 89.00%\n",
    "  Confusion Matrix:\n",
    "[[316  28   1  27]\n",
    " [ 43 690   5  30]\n",
    " [ 11  27 309   5]\n",
    " [ 12   9   0 287]]\n",
    "\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1926 | Train Acc: 93.02%\n",
    "  Val   Loss: 0.3289 | Val   Acc: 89.61%\n",
    "  Confusion Matrix:\n",
    "[[303  49   3  17]\n",
    " [ 25 707  20  16]\n",
    " [  5  16 327   4]\n",
    " [ 10  18   4 276]]\n",
    "\n",
    "Test Results\n",
    "  Test Loss:   0.4768 | Accuracy: 84.82%\n",
    "  Confusion Matrix:\n",
    "[[442 151  28  74]\n",
    " [ 29 995  40  22]\n",
    " [ 10  21 762   6]\n",
    " [ 45  78  17 712]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc2925a4-8c04-407a-87cd-68a7d77a7089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/jacks/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|| 97.8M/97.8M [00:00<00:00, 120MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.4458 | Train Acc: 83.95%\n",
      "  Val   Loss: 0.3161 | Val   Acc: 89.22%\n",
      "  Confusion Matrix:\n",
      "[[328  27   4  13]\n",
      " [ 45 702  14   7]\n",
      " [  9  18 321   4]\n",
      " [ 33  18   2 255]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2574 | Train Acc: 91.04%\n",
      "  Val   Loss: 0.2884 | Val   Acc: 89.56%\n",
      "  Confusion Matrix:\n",
      "[[317  29   8  18]\n",
      " [ 30 695  30  13]\n",
      " [  5  17 326   4]\n",
      " [ 21  13   0 274]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1943 | Train Acc: 93.16%\n",
      "  Val   Loss: 0.3003 | Val   Acc: 89.78%\n",
      "  Confusion Matrix:\n",
      "[[322  33   6  11]\n",
      " [ 29 710  17  12]\n",
      " [  6  22 320   4]\n",
      " [ 25  18   1 264]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.1584 | Train Acc: 94.48%\n",
      "  Val   Loss: 0.3171 | Val   Acc: 89.72%\n",
      "  Confusion Matrix:\n",
      "[[318  31   6  17]\n",
      " [ 30 705  22  11]\n",
      " [  6  19 325   2]\n",
      " [ 19  18   4 267]]\n",
      "\n",
      "Current LR: 1e-05\n",
      "Test Results\n",
      "  Test Loss:   0.4935 | Accuracy: 84.99%\n",
      "  Confusion Matrix:\n",
      "[[495 120  29  51]\n",
      " [ 38 991  43  14]\n",
      " [ 17  21 756   5]\n",
      " [ 53 100  24 675]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition (Gated Fusion with ResNet50)\n",
    "# ======================\n",
    "class MultiModalClassifierGated(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifierGated, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Image model: ResNet50 ----\n",
    "        self.image_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        # ResNet50 outputs 2048-dimensional features.\n",
    "        if 2048 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(2048, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Gating Mechanism ----\n",
    "        # Computes element-wise weights for text and image features.\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # use [CLS] token representation\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # project to fusion_dim\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # get 2048-dim features from ResNet50\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # project to fusion_dim\n",
    "        \n",
    "        # --- Gating Fusion ---\n",
    "        combined_feats = torch.cat([text_feat, image_feat], dim=1)  # (batch, fusion_dim*2)\n",
    "        gate_weights = self.gate(combined_feats)  # (batch, fusion_dim)\n",
    "        fused_feat = gate_weights * text_feat + (1 - gate_weights) * image_feat  # element-wise fusion\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation with Enhanced Augmentation\n",
    "# ======================\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as before\n",
    "\n",
    "# Define dataset paths.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the gating fusion model with ResNet50.\n",
    "model = MultiModalClassifierGated(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler (without verbose parameter).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afeb409-77d0-43f1-82bb-7cd736389ed3",
   "metadata": {},
   "source": [
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.4458 | Train Acc: 83.95%\n",
    "  Val   Loss: 0.3161 | Val   Acc: 89.22%\n",
    "  Confusion Matrix:\n",
    "[[328  27   4  13]\n",
    " [ 45 702  14   7]\n",
    " [  9  18 321   4]\n",
    " [ 33  18   2 255]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2574 | Train Acc: 91.04%\n",
    "  Val   Loss: 0.2884 | Val   Acc: 89.56%\n",
    "  Confusion Matrix:\n",
    "[[317  29   8  18]\n",
    " [ 30 695  30  13]\n",
    " [  5  17 326   4]\n",
    " [ 21  13   0 274]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1943 | Train Acc: 93.16%\n",
    "  Val   Loss: 0.3003 | Val   Acc: 89.78%\n",
    "  Confusion Matrix:\n",
    "[[322  33   6  11]\n",
    " [ 29 710  17  12]\n",
    " [  6  22 320   4]\n",
    " [ 25  18   1 264]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1584 | Train Acc: 94.48%\n",
    "  Val   Loss: 0.3171 | Val   Acc: 89.72%\n",
    "  Confusion Matrix:\n",
    "[[318  31   6  17]\n",
    " [ 30 705  22  11]\n",
    " [  6  19 325   2]\n",
    " [ 19  18   4 267]]\n",
    "\n",
    "Current LR: 1e-05\n",
    "Test Results\n",
    "  Test Loss:   0.4935 | Accuracy: 84.99%\n",
    "  Confusion Matrix:\n",
    "[[495 120  29  51]\n",
    " [ 38 991  43  14]\n",
    " [ 17  21 756   5]\n",
    " [ 53 100  24 675]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d8ec4-efd0-46ee-8e8c-031d0705e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# ======================\n",
    "# 1. Custom Multimodal Dataset\n",
    "# ======================\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                # sort files to ensure consistent order\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        # Extract text from file name\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "            \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ======================\n",
    "# 2. Multimodal Model Definition (Gated Fusion with ResNet50)\n",
    "# ======================\n",
    "class MultiModalClassifierGated(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze all image model parameters except those in layer4.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifierGated, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Image model: ResNet50 ----\n",
    "        self.image_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        if freeze_image_layers:\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('layer4'):\n",
    "                    param.requires_grad = False\n",
    "        # ResNet50 outputs 2048-dimensional features.\n",
    "        if 2048 != fusion_dim:\n",
    "            self.image_projection = nn.Linear(2048, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)  # increased dropout\n",
    "        \n",
    "        # ---- Gating Mechanism ----\n",
    "        # Computes element-wise weights for text and image features.\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # --- Process text ---\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # use [CLS] token representation\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)  # project to fusion_dim\n",
    "        \n",
    "        # --- Process image ---\n",
    "        image_feat = self.image_model(image)  # get 2048-dim features from ResNet50\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)  # project to fusion_dim\n",
    "        \n",
    "        # --- Gating Fusion ---\n",
    "        combined_feats = torch.cat([text_feat, image_feat], dim=1)  # (batch, fusion_dim*2)\n",
    "        gate_weights = self.gate(combined_feats)  # (batch, fusion_dim)\n",
    "        fused_feat = gate_weights * text_feat + (1 - gate_weights) * image_feat  # element-wise fusion\n",
    "        \n",
    "        # --- Classification ---\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n",
    "\n",
    "# ======================\n",
    "# 3. Data Preparation with Enhanced Augmentation\n",
    "# ======================\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24  # as before\n",
    "\n",
    "# Define dataset paths.\n",
    "TRAIN_PATH = './CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './CVPR_2024_dataset_Test'\n",
    "\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "\n",
    "# ======================\n",
    "# Create DataLoaders with Balancing for Training Data\n",
    "# ======================\n",
    "# Compute sample weights based on class frequencies in the training set.\n",
    "train_labels = [sample[2] for sample in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts  # Inverse frequency for each class.\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "# Create the sampler for balanced sampling.\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Use the sampler in the training DataLoader.\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# ======================\n",
    "# 4. Model, Optimizer, and Loss Function Setup\n",
    "# ======================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the gating fusion model with ResNet50.\n",
    "model = MultiModalClassifierGated(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Use weight decay for regularization.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler (without verbose parameter).\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "# ======================\n",
    "# 5. Training and Evaluation Loops\n",
    "# ======================\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item(), accuracy=running_corrects / total_samples)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "\n",
    "EPOCHS = 4\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "\n",
    "test_loss, test_acc, test_cm = evaluate_model(model, test_loader, criterion, device)\n",
    "print(\"Test Results\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca583315-8420-4029-b3d7-75345a2d27c2",
   "metadata": {},
   "source": [
    "/home/jacks/miniconda3/envs/enen_645/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
    "  from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "Epoch 1/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.4106 | Train Acc: 85.17%\n",
    "  Val   Loss: 0.3581 | Val   Acc: 86.89%\n",
    "  Confusion Matrix:\n",
    "[[324  19   5  24]\n",
    " [ 68 646  34  20]\n",
    " [  8  14 327   3]\n",
    " [ 22  16   3 267]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 2/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.2303 | Train Acc: 92.28%\n",
    "  Val   Loss: 0.3172 | Val   Acc: 90.17%\n",
    "  Confusion Matrix:\n",
    "[[333  21   1  17]\n",
    " [ 46 692  13  17]\n",
    " [ 11  15 321   5]\n",
    " [ 13  18   0 277]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 3/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1822 | Train Acc: 93.75%\n",
    "  Val   Loss: 0.3428 | Val   Acc: 90.22%\n",
    "  Confusion Matrix:\n",
    "[[326  23   6  17]\n",
    " [ 34 708  12  14]\n",
    " [  7  20 320   5]\n",
    " [ 21  16   1 270]]\n",
    "\n",
    "Current LR: 2e-05\n",
    "Epoch 4/4\n",
    "\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1457 | Train Acc: 95.00%\n",
    "  Val   Loss: 0.3300 | Val   Acc: 90.22%\n",
    "  Confusion Matrix:\n",
    "[[324  25   6  17]\n",
    " [ 32 712  15   9]\n",
    " [  7  24 317   4]\n",
    " [ 15  21   1 271]]\n",
    "\n",
    "Current LR: 1e-05\n",
    "Test Results\n",
    "  Test Loss:   0.5175 | Accuracy: 84.47%\n",
    "  Confusion Matrix:\n",
    "[[488 106  23  78]\n",
    " [ 51 987  37  11]\n",
    " [ 23  28 739   9]\n",
    " [ 50 102  15 685]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:enen_645]",
   "language": "python",
   "name": "conda-env-enen_645-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
