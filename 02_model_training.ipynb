{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ffa9b93-17ff-4d97-b60a-733257d96eff",
   "metadata": {},
   "source": [
    "### System Requirements:\n",
    "* The model was trained on a PC with an RTX 2080 with 8Gb of VRAM. The model during training took roughly ~7 Gb of VRAM with a batch size of 32.\n",
    "* The weights for the image model selected are 2461.6 MB and the weights for the text model is ~200 MB. Therefore, a minmimum VRAM of 4 Gb is probably required for training the model configuration. \n",
    "\n",
    "### Acknowledgement: \n",
    "* The following code is developed based on the tutorial of ENEN//ENEL 645 from Roberto Souza. Namely the [text-classification](https://github.com/rmsouza01/ENEL-ENEN-645-W2025/blob/main/Tutorials/garbage_classification_text.ipynb) and [image-classification](https://github.com/rmsouza01/ENEL-ENEN-645-W2025/blob/main/Tutorials/garbage_classifier_images.ipynb).\n",
    "* ChatGPT was also used in the development and refinement of the code. \n",
    "\n",
    "### Overview:\n",
    "* The model presented in this notebook is based on the results from [01_model_evaluation](01_model_evaluation.ipynb).\n",
    "* The DistilBert model was used for the text component of the classificaiton as it was found to be adequate accuarcy at ~89% based on the provided tutorial.\n",
    "* The [RegNET model](https://pytorch.org/vision/main/models/generated/torchvision.models.regnet_y_128gf.html#torchvision.models.RegNet_Y_128GF_Weights) was used due to its superior perfomrance compared to other pre-trained models avialable through Pytorch and through our evaluated results.\n",
    "* The [Gated Fusion mechansim](https://arxiv.org/abs/1904.01803) was used as it was found to be superior to other methods trialed.\n",
    "\n",
    "#### **Model Weights:**\n",
    "* The model weights are not included on this repo or provided through Dropbox as they were 2.64 Gb after training. Therefore, if you require them, please contact me at jackson.cooper@ucalgary.ca and I would be happy to provide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1838bd-3402-47e5-a82a-9d95d7ed900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2aeaab6-407d-4483-8d69-f8198f995d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is read in based on the expected pre-structured classes and associated labels in the name. \n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'file_name': os.path.basename(image_path)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a88261b-459c-430e-b18c-de58b136d3d0",
   "metadata": {},
   "source": [
    "After defining our data class, we can create the model to be trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e337be96-ca92-49e8-820c-6d832b2d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model uses DistilBERT for text and a RegNet model for images.\n",
    "# A gating mechanism fuses the two modalities.\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze most image model parameters except for the last stage ('s4'),\n",
    "        ensuring that only one layer (i.e. the s4 block) is tuned in the vision branch.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Image model: RegNet_Y_128GF ----\n",
    "        self.image_model = models.regnet_y_128gf(weights=models.RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "        # Remove the final fc layer to obtain features.\n",
    "        image_feature_dim = self.image_model.fc.in_features\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        \n",
    "        if freeze_image_layers:\n",
    "            # Freeze all parameters except those in the last stage ('s4').\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('s4'):\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # If necessary, project image features to fusion_dim.\n",
    "        if image_feature_dim != fusion_dim:\n",
    "            self.image_projection = nn.Linear(image_feature_dim, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    " \n",
    "        # ---- Gating Mechanism for Fusion ----\n",
    "        # This mechanism learns how to weight text and image features.\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dim, fusion_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fusion_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Text branch\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)\n",
    "        \n",
    "        # Image branch\n",
    "        image_feat = self.image_model(image)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)\n",
    "        \n",
    "        # Fusion via gating mechanism\n",
    "        # Pretty much this fusion mechanism balances between the two models to limit potential noise from either model\n",
    "        gate_weights = self.gate(torch.cat([text_feat, image_feat], dim=1))\n",
    "        fused_feat = gate_weights * text_feat + (1 - gate_weights) * image_feat\n",
    "        fused_feat = self.fusion_dropout(fused_feat)\n",
    "        \n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c1c80-4f6f-4591-9dcd-dab14f843046",
   "metadata": {},
   "source": [
    "Several data augnmentations were applied to the image dataset to help improve generalizability to new imagery. This includes geometric changes such as spatial transformations (no camera distortion was applied but could be useful if your new imagery is not expected to be calibrated). As well as radiometric adjustments. It is worth noting, that in the dataset, several examples looked to have a masked background, i.e a white background, so applying masking as an additioanl augmentation to the dataset may be valuable. \n",
    "\n",
    "You could potentially apply transformations to the text, such as the use of syonymns, or translations, however, I have not worked with that before, and am unaware how useful it would be in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836653d3-2487-4251-b85c-19a828a4a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations and initialize the tokenizer.\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24\n",
    "\n",
    "# Paths for train, validation, and test datasets.\n",
    "TRAIN_PATH = './garbage_data/CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './garbage_data/CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14023fad-19ac-447a-9532-1db02a1af26c",
   "metadata": {},
   "source": [
    "Using a weighted sampler to ensure we are balancing between the classes, as the blue cart class is nearly 2x representation of the other individual classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40878067-db34-4d77-a163-c2228a9a657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a WeightedRandomSampler to handle class imbalance in the training data.\n",
    "train_labels = [sample[2] for sample in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ab0b0c-3264-4c28-8bf1-99a497c247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the multimodal classifier.\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer, loss function, and learning rate scheduler.\n",
    "# Basing off of the tutorial, and trialed parameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515dac3a-418e-4459-9995-410b92c9b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions handle a single training epoch and evaluation on validation data.\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Batch metrics\n",
    "        batch_loss = loss.item()\n",
    "        batch_accuracy = (torch.sum(preds == labels).item() / labels.size(0))\n",
    "        batch_losses.append(batch_loss)\n",
    "        batch_accs.append(batch_accuracy)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=batch_loss, accuracy=batch_accuracy)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc, batch_losses, batch_accs\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Batch metrics for evaluation\n",
    "            batch_loss = loss.item()\n",
    "            batch_accuracy = (torch.sum(outputs.argmax(dim=1) == labels).item() / labels.size(0))\n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_accs.append(batch_accuracy)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=batch_loss, accuracy=batch_accuracy)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm, batch_losses, batch_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15393d-3b63-4699-b3aa-dcbf889f411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize metric tracking lists.\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "train_batches_losses = []\n",
    "train_batches_accs = []\n",
    "val_batches_losses = []\n",
    "val_batches_accs = []\n",
    "\n",
    "# Early stopping parameters.\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc, epoch_train_batch_losses, epoch_train_batch_accs = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm, epoch_val_batch_losses, epoch_val_batch_accs = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    train_batches_losses.append(epoch_train_batch_losses)\n",
    "    train_batches_accs.append(epoch_train_batch_accs)\n",
    "    val_batches_losses.append(epoch_val_batch_losses)\n",
    "    val_batches_accs.append(epoch_val_batch_accs)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "        print(\"Model improved. Saving model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e6bbb1-c7a2-4aeb-b7f5-002e24a65532",
   "metadata": {},
   "source": [
    "### Training Results from the Current Evalauted Model Below\n",
    "\n",
    "Epoch 1/10\n",
    "\n",
    "  Train Loss: 0.5469 | Train Acc: 79.01%\n",
    "  Val   Loss: 0.3523 | Val   Acc: 87.61%\n",
    "  Confusion Matrix:\n",
    "[[313  27   3  29]\n",
    " [ 54 665  22  27]\n",
    " [ 10  16 319   7]\n",
    " [ 11  15   2 280]]\n",
    "\n",
    "Current LR: 1.9271177344544742e-05\n",
    "Model improved. Saving model. \n",
    "\n",
    "Epoch 2/10\n",
    "\n",
    "\n",
    "  Train Loss: 0.2809 | Train Acc: 90.54%\n",
    "  Val   Loss: 0.3587 | Val   Acc: 86.94%\n",
    "  Confusion Matrix:\n",
    "[[339  15   2  16]\n",
    " [ 86 634  28  20]\n",
    " [ 16  11 321   4]\n",
    " [ 24  12   1 271]]\n",
    "\n",
    "Current LR: 1.9258305525889206e-05\n",
    "No improvement in validation loss. Patience counter: 1/2\n",
    "\n",
    "Epoch 3/10\n",
    "\n",
    "  Train Loss: 0.2228 | Train Acc: 92.35%\n",
    "  Val   Loss: 0.2998 | Val   Acc: 89.56%\n",
    "  Confusion Matrix:\n",
    "[[323  30   3  16]\n",
    " [ 42 697  16  13]\n",
    " [  4  19 325   4]\n",
    " [ 25  15   1 267]]\n",
    "\n",
    "Current LR: 1.9378124908687465e-05\n",
    "Model improved. Saving model.\n",
    "\n",
    "Epoch 4/10\n",
    "\n",
    "  Train Loss: 0.1875 | Train Acc: 93.31%\n",
    "  Val   Loss: 0.3329 | Val   Acc: 89.56%\n",
    "  Confusion Matrix:\n",
    "[[329  24   3  16]\n",
    " [ 41 685  28  14]\n",
    " [  7  14 327   4]\n",
    " [ 19  14   4 271]]\n",
    "\n",
    "Current LR: 1.9310686470668903e-05\n",
    "No improvement in validation loss. Patience counter: 1/2\n",
    "\n",
    "Epoch 5/10\n",
    "                                                                                                                        \n",
    "\n",
    "  Train Loss: 0.1679 | Train Acc: 94.26%\n",
    "  Val   Loss: 0.3080 | Val   Acc: 89.61%\n",
    "  Confusion Matrix:\n",
    "[[320  26   4  22]\n",
    " [ 37 688  24  19]\n",
    " [  4  17 326   5]\n",
    " [ 14  12   3 279]]\n",
    "\n",
    "Current LR: 1.936136835732785e-05\n",
    "No improvement in validation loss. Patience counter: 2/2\n",
    "Early stopping triggered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50018488-99f4-452c-9489-4389b026f60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results (Best Model)\n",
      "  Test Loss:   0.4336 | Accuracy: 85.64%\n",
      "  Confusion Matrix:\n",
      "[[527  93  21  54]\n",
      " [ 61 981  31  13]\n",
      " [ 23  17 756   3]\n",
      " [ 82  80  15 675]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Loading the best saved model before evaluating on the test set\n",
    "model.load_state_dict(torch.load('best_multimodal_model.pth'))\n",
    "model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Now evaluate on the test set\n",
    "test_loss, test_acc, test_cm, test_batch_losses, test_batch_accs = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"Test Results (Best Model)\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de19b53-98f0-4a7c-bbe8-6de6cdeeba69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3851f2a2-e45f-4b80-9336-08820ca6a9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8563519813519813\n",
      "Precision per class: [0.76046176 0.83774552 0.91859052 0.90604027]\n",
      "Recall per class: [0.75827338 0.90331492 0.94618273 0.79225352]\n",
      "F1-scores per class: [0.75936599 0.86929553 0.93218249 0.845335  ]\n",
      "Macro Precision: 0.8557095170123266\n",
      "Macro Recall: 0.8500061369898274\n",
      "Macro F1: 0.8515447532881424\n",
      "Weighted Precision: 0.8578708306500027\n",
      "Weighted Recall: 0.8563519813519813\n",
      "Weighted F1: 0.855726555640676\n"
     ]
    }
   ],
   "source": [
    "# Overall accuracy: Sum of diagonal elements divided by total sum\n",
    "accuracy = np.trace(test_cm) / np.sum(test_cm)\n",
    "\n",
    "# Precision for each class: TP / (TP + FP)\n",
    "precision = np.diag(test_cm) / np.sum(test_cm, axis=0)\n",
    "\n",
    "# Recall for each class: TP / (TP + FN)\n",
    "recall = np.diag(test_cm) / np.sum(test_cm, axis=1)\n",
    "\n",
    "# F1-score for each class: harmonic mean of precision and recall\n",
    "f1_scores = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "# Macro averages: simple average over classes\n",
    "macro_precision = np.mean(precision)\n",
    "macro_recall = np.mean(recall)\n",
    "macro_f1 = np.mean(f1_scores)\n",
    "\n",
    "# Weighted averages: weighted by the support (true instances per class)\n",
    "support = np.sum(test_cm, axis=1)\n",
    "weighted_precision = np.sum(precision * support) / np.sum(support)\n",
    "weighted_recall = np.sum(recall * support) / np.sum(support)\n",
    "weighted_f1 = np.sum(f1_scores * support) / np.sum(support)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1-scores per class:\", f1_scores)\n",
    "print(\"Macro Precision:\", macro_precision)\n",
    "print(\"Macro Recall:\", macro_recall)\n",
    "print(\"Macro F1:\", macro_f1)\n",
    "print(\"Weighted Precision:\", weighted_precision)\n",
    "print(\"Weighted Recall:\", weighted_recall)\n",
    "print(\"Weighted F1:\", weighted_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b2e14-0011-4e31-b558-5923e7204f27",
   "metadata": {},
   "source": [
    "To better understand the performance of classificaiton, I plot sampled prediciton results for the different classes (both correct and incorrect). These results can be seen in the accompanying [PDF](evaluated_results.pdf). \n",
    "\n",
    "Broadly speaking, these results to be failry resonable, as several of the objects I would have difficulty classfying myself. In particular, some items such as the dryer sheet, and the binder in the TTR class, I would personally have put into the Black bin. \n",
    "\n",
    "Therefore, the results determined with our model may be best compared to a human expert in classfying. This would provide a better understanding of how well the model is actually performing in the real world. \n",
    "\n",
    "It would also potentially be valuable in auding the dataset of the model to ensure that items are in the appropraite classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "944b832f-7e9f-474c-979c-b5a70547e6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from matplotlib.backends.backend_pdf import PdfPages  \n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Helper: Denormalize an image tensor for visualization.\n",
    "def denormalize(img_tensor):\n",
    "    # Mean and std must match the ones used during normalization.\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = img_tensor * std + mean\n",
    "    img = img.clamp(0, 1)\n",
    "    return img.permute(1, 2, 0).numpy()\n",
    "\n",
    "# Create a subset from the test dataset to speed up inference.\n",
    "subset_size = min(100, len(test_dataset))\n",
    "subset_indices = np.random.choice(len(test_dataset), size=subset_size, replace=False)\n",
    "subset_test = Subset(test_dataset, subset_indices)\n",
    "subset_test_loader = DataLoader(subset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "# Run inference on the subset.\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_file_names = []\n",
    "all_images = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in subset_test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        file_names = batch['file_name']  # 'file_name' must be returned by your dataset\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        \n",
    "        for i in range(len(labels)):\n",
    "            all_preds.append(preds[i].item())\n",
    "            all_labels.append(labels[i].item())\n",
    "            all_file_names.append(file_names[i])\n",
    "            all_images.append(images[i].cpu())\n",
    "\n",
    "# Create reverse mapping from label index to class name.\n",
    "idx_to_class = {v: k for k, v in test_dataset.label_map.items()}\n",
    "\n",
    "# Open a PdfPages object to save multiple figures into a single PDF.\n",
    "with PdfPages('evaluated_results.pdf') as pdf:\n",
    "    # For each class, sample up to N=4 correct and N=4 incorrect predictions.\n",
    "    N = 4\n",
    "    for class_idx in sorted(idx_to_class.keys()):\n",
    "        # Get indices for samples of the current class.\n",
    "        indices = [i for i, label in enumerate(all_labels) if label == class_idx]\n",
    "        correct_indices = [i for i in indices if all_preds[i] == class_idx]\n",
    "        incorrect_indices = [i for i in indices if all_preds[i] != class_idx]\n",
    "        \n",
    "        # Sample up to N examples from each group.\n",
    "        sampled_correct = random.sample(correct_indices, min(N, len(correct_indices)))\n",
    "        sampled_incorrect = random.sample(incorrect_indices, min(N, len(incorrect_indices)))\n",
    "        \n",
    "        # Plotting: Two rows (correct on top, incorrect on bottom) with N columns.\n",
    "        fig, axes = plt.subplots(2, N, figsize=(N * 4, 8))\n",
    "        fig.suptitle(f\"Class: {idx_to_class[class_idx]}\", fontsize=16)\n",
    "        \n",
    "        # Top row: Correct predictions.\n",
    "        for j in range(N):\n",
    "            ax = axes[0, j] if N > 1 else axes[0]\n",
    "            if j < len(sampled_correct):\n",
    "                idx = sampled_correct[j]\n",
    "                img = denormalize(all_images[idx])\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"File: {all_file_names[idx]}\\nPred: {idx_to_class[all_preds[idx]]}\", fontsize=10)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "        axes[0, 0].set_ylabel(\"Correct\", fontsize=14)\n",
    "        \n",
    "        # Bottom row: Incorrect predictions.\n",
    "        for j in range(N):\n",
    "            ax = axes[1, j] if N > 1 else axes[1]\n",
    "            if j < len(sampled_incorrect):\n",
    "                idx = sampled_incorrect[j]\n",
    "                img = denormalize(all_images[idx])\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "                ax.set_title(f\"File: {all_file_names[idx]}\\nPred: {idx_to_class[all_preds[idx]]}\", fontsize=10)\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        # Save the current figure as a page in the PDF.\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:enen_645]",
   "language": "python",
   "name": "conda-env-enen_645-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
