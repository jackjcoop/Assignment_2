{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1838bd-3402-47e5-a82a-9d95d7ed900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Dependencies\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2aeaab6-407d-4483-8d69-f8198f995d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Custom Multimodal Dataset\n",
    "# This dataset expects a folder structure where each subfolder corresponds to a class.\n",
    "# The text is extracted from the image file names.\n",
    "class MultiModalDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, max_len, image_transform=None):\n",
    "        \"\"\"\n",
    "        Expects a folder structure:\n",
    "            root_dir/\n",
    "                class1/\n",
    "                    image1.jpg\n",
    "                    image2.jpg\n",
    "                    ...\n",
    "                class2/\n",
    "                    ...\n",
    "        The text is extracted from the file name (by removing extension, replacing underscores, etc.)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.image_transform = image_transform\n",
    "        self.samples = []  # each element is a tuple (image_path, text, label)\n",
    "        self.class_folders = sorted(os.listdir(root_dir))\n",
    "        self.label_map = {cls: idx for idx, cls in enumerate(self.class_folders)}\n",
    "        \n",
    "        for cls in self.class_folders:\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            if os.path.isdir(cls_path):\n",
    "                for file in sorted(os.listdir(cls_path)):\n",
    "                    file_path = os.path.join(cls_path, file)\n",
    "                    if os.path.isfile(file_path):\n",
    "                        file_name_no_ext, _ = os.path.splitext(file)\n",
    "                        text = file_name_no_ext.replace('_', ' ')\n",
    "                        text = re.sub(r'\\d+', '', text)\n",
    "                        self.samples.append((file_path, text, self.label_map[cls]))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path, text, label = self.samples[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'image': image,\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e337be96-ca92-49e8-820c-6d832b2d3640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Multimodal Model Definition\n",
    "# The model uses DistilBERT for text and a RegNet model for images.\n",
    "# A MLP layer fuses the two modalities.\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, fusion_dim=512, text_model_name='distilbert-base-uncased', freeze_image_layers=True):\n",
    "        \"\"\"\n",
    "        fusion_dim: common embedding dimension for both modalities.\n",
    "        freeze_image_layers: if True, freeze most image model parameters except for the last stage ('s4'),\n",
    "        ensuring that only one layer (i.e. the s4 block) is tuned in the vision branch.\n",
    "        \"\"\"\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # ---- Text model: DistilBERT ----\n",
    "        self.text_model = DistilBertModel.from_pretrained(text_model_name)\n",
    "        self.text_projection = nn.Linear(self.text_model.config.hidden_size, fusion_dim)\n",
    "        self.text_dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # ---- Image model: RegNet_Y_128GF ----\n",
    "        self.image_model = models.regnet_y_128gf(weights=models.RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "        # Remove the final fc layer to obtain features.\n",
    "        image_feature_dim = self.image_model.fc.in_features\n",
    "        self.image_model.fc = nn.Identity()\n",
    "        \n",
    "        if freeze_image_layers:\n",
    "            # Freeze all parameters except those in the last stage ('s4').\n",
    "            for name, param in self.image_model.named_parameters():\n",
    "                if not name.startswith('s4'):\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # If necessary, project image features to fusion_dim.\n",
    "        if image_feature_dim != fusion_dim:\n",
    "            self.image_projection = nn.Linear(image_feature_dim, fusion_dim)\n",
    "        else:\n",
    "            self.image_projection = nn.Identity()\n",
    "        self.image_dropout = nn.Dropout(0.5)\n",
    " \n",
    "        # ---- MLP-based Fusion Mechanism ----\n",
    "        # The MLP takes concatenated text and image features and learns a fused representation.\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_dim * 2, fusion_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fusion_dim, fusion_dim)\n",
    "        )\n",
    "        \n",
    "        # ---- Classification Head ----\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Text branch: extract [CLS] token representation from DistilBERT.\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_feat = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_feat = self.text_dropout(text_feat)\n",
    "        text_feat = self.text_projection(text_feat)\n",
    "        \n",
    "        # Image branch: extract features from the image model.\n",
    "        image_feat = self.image_model(image)\n",
    "        image_feat = self.image_dropout(image_feat)\n",
    "        image_feat = self.image_projection(image_feat)\n",
    "        \n",
    "        # Fusion: concatenate text and image features and pass through the MLP.\n",
    "        fused_feat = self.fusion(torch.cat([text_feat, image_feat], dim=1))\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(fused_feat)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "836653d3-2487-4251-b85c-19a828a4a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Preparation and Augmentation\n",
    "# Define image transformations and initialize the tokenizer.\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_len = 24\n",
    "\n",
    "# Paths for train, validation, and test datasets.\n",
    "TRAIN_PATH = './garbage_data/CVPR_2024_dataset_Train'\n",
    "VAL_PATH = './garbage_data/CVPR_2024_dataset_Val'\n",
    "TEST_PATH = './garbage_data/CVPR_2024_dataset_Test'\n",
    "\n",
    "# Create dataset instances.\n",
    "train_dataset = MultiModalDataset(TRAIN_PATH, tokenizer, max_len, image_transform=train_image_transform)\n",
    "val_dataset = MultiModalDataset(VAL_PATH, tokenizer, max_len, image_transform=val_test_transform)\n",
    "test_dataset = MultiModalDataset(TEST_PATH, tokenizer, max_len, image_transform=val_test_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40878067-db34-4d77-a163-c2228a9a657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: DataLoader Setup\n",
    "# Use a WeightedRandomSampler to handle class imbalance in the training data.\n",
    "train_labels = [sample[2] for sample in train_dataset.samples]\n",
    "class_counts = np.bincount(train_labels)\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ab0b0c-3264-4c28-8bf1-99a497c247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model, Optimizer, and Loss Function Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 4\n",
    "\n",
    "# Instantiate the multimodal classifier.\n",
    "model = MultiModalClassifier(num_classes=num_classes, fusion_dim=512, freeze_image_layers=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the optimizer, loss function, and learning rate scheduler.\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515dac3a-418e-4459-9995-410b92c9b35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training and Evaluation with Early Stopping\n",
    "# The following functions handle a single training epoch and evaluation on validation data.\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "        \n",
    "        # Batch metrics\n",
    "        batch_loss = loss.item()\n",
    "        batch_accuracy = (torch.sum(preds == labels).item() / labels.size(0))\n",
    "        batch_losses.append(batch_loss)\n",
    "        batch_accs.append(batch_accuracy)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=batch_loss, accuracy=batch_accuracy)\n",
    "    \n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    return epoch_loss, epoch_acc, batch_losses, batch_accs\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    batch_losses = []\n",
    "    batch_accs = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Batch metrics for evaluation\n",
    "            batch_loss = loss.item()\n",
    "            batch_accuracy = (torch.sum(outputs.argmax(dim=1) == labels).item() / labels.size(0))\n",
    "            batch_losses.append(batch_loss)\n",
    "            batch_accs.append(batch_accuracy)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=batch_loss, accuracy=batch_accuracy)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm, batch_losses, batch_accs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15393d-3b63-4699-b3aa-dcbf889f411a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.6304 | Train Acc: 76.43%\n",
      "  Val   Loss: 0.3683 | Val   Acc: 87.44%\n",
      "  Confusion Matrix:\n",
      "[[303  28   3  38]\n",
      " [ 49 666  19  34]\n",
      " [  9  18 315  10]\n",
      " [  3  12   3 290]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Model improved. Saving model.\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2703 | Train Acc: 90.86%\n",
      "  Val   Loss: 0.3267 | Val   Acc: 88.06%\n",
      "  Confusion Matrix:\n",
      "[[321  25   3  23]\n",
      " [ 63 670  18  17]\n",
      " [ 13  18 318   3]\n",
      " [ 12  17   3 276]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "Model improved. Saving model.\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 0.2195 | Train Acc: 92.52%\n",
      "  Val   Loss: 0.3296 | Val   Acc: 88.72%\n",
      "  Confusion Matrix:\n",
      "[[327  19   1  25]\n",
      " [ 63 670  16  19]\n",
      " [ 12  20 316   4]\n",
      " [  7  15   2 284]]\n",
      "\n",
      "Current LR: 2e-05\n",
      "No improvement in validation loss. Patience counter: 1/2\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|████▉                                      | 42/364 [01:41<12:02,  2.24s/it, accuracy=0.969, loss=0.0872]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize metric tracking lists.\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "train_batches_losses = []\n",
    "train_batches_accs = []\n",
    "val_batches_losses = []\n",
    "val_batches_accs = []\n",
    "\n",
    "# Early stopping parameters.\n",
    "EPOCHS = 10\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    train_loss, train_acc, epoch_train_batch_losses, epoch_train_batch_accs = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc, val_cm, epoch_val_batch_losses, epoch_val_batch_accs = evaluate_model(model, val_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    train_batches_losses.append(epoch_train_batch_losses)\n",
    "    train_batches_accs.append(epoch_train_batch_accs)\n",
    "    val_batches_losses.append(epoch_val_batch_losses)\n",
    "    val_batches_accs.append(epoch_val_batch_accs)\n",
    "    \n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc*100:.2f}%\")\n",
    "    print(f\"  Confusion Matrix:\\n{val_cm}\\n\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    print(\"Current LR:\", scheduler.optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_multimodal_model.pth')\n",
    "        print(\"Model improved. Saving model.\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50018488-99f4-452c-9489-4389b026f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model before evaluating on the test set\n",
    "model.load_state_dict(torch.load('best_multimodal_model.pth'))\n",
    "model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Now evaluate on the test set\n",
    "test_loss, test_acc, test_cm, test_batch_losses, test_batch_accs = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(\"Test Results (Best Model)\")\n",
    "print(f\"  Test Loss:   {test_loss:.4f} | Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"  Confusion Matrix:\\n{test_cm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de19b53-98f0-4a7c-bbe8-6de6cdeeba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8dad5-1f52-495f-8f2a-6e824e4520db",
   "metadata": {},
   "outputs": [],
   "source": [
    "~7,4gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b832f-7e9f-474c-979c-b5a70547e6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:enen_645]",
   "language": "python",
   "name": "conda-env-enen_645-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
